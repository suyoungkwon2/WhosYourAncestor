#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ Colab í¬ë¡¤ëŸ¬ v4 - 2ë‹¨ê³„ í™•ì¥ êµ­ê°€ ìˆ˜ì§‘
"""

import os
import sys
import subprocess
import time
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from PIL import Image
import io
import re

def install_requirements():
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜"""
    print("ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...")
    
    packages = [
        "selenium==4.15.2",
        "pillow==10.1.0", 
        "requests==2.31.0",
        "webdriver-manager==4.0.1"
    ]
    
    for package in packages:
        try:
            subprocess.run([sys.executable, "-m", "pip", "install", package], 
                         check=True, capture_output=True)
            print(f"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ")
        except subprocess.CalledProcessError as e:
            print(f"âŒ {package} ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
    
    # Chrome ë“œë¼ì´ë²„ ì„¤ì¹˜
    try:
        subprocess.run(["apt-get", "update"], check=True, capture_output=True)
        subprocess.run(["apt-get", "install", "-y", "chromium-chromedriver"], 
                      check=True, capture_output=True)
        print("âœ… Chrome ë“œë¼ì´ë²„ ì„¤ì¹˜ ì™„ë£Œ")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Chrome ë“œë¼ì´ë²„ ì„¤ì¹˜ ì‹¤íŒ¨: {e}")

def setup_environment():
    """í™˜ê²½ ì„¤ì •"""
    os.environ['PATH'] += ':/usr/bin/chromedriver'
    
    try:
        import torch
        if torch.cuda.is_available():
            print("ğŸš€ GPU ê°€ì† ì‚¬ìš© ê°€ëŠ¥")
        else:
            print("âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    except ImportError:
        print("âš ï¸ PyTorch ë¯¸ì„¤ì¹˜ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")

class HighQualityColabCrawler:
    def __init__(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.driver = None
        self.setup_driver()
    
    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            print("âœ… Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def scroll_page(self, max_scrolls=8):
        """í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ì´ë¯¸ì§€ ë¡œë“œ"""
        for i in range(max_scrolls):
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            print(f"ğŸ“œ ìŠ¤í¬ë¡¤ {i+1}/{max_scrolls}")
    
    def find_image_elements(self):
        """ë‹¤ì–‘í•œ CSS ì„ íƒìë¡œ ì´ë¯¸ì§€ ìš”ì†Œ ì°¾ê¸°"""
        selectors = [
            "img.rg_i",  # ê¸°ì¡´ ì„ íƒì
            "img[data-src]",  # ì§€ì—° ë¡œë”© ì´ë¯¸ì§€
            "img[src*='http']",  # HTTP ì´ë¯¸ì§€
            ".rg_i",  # ì´ë¯¸ì§€ ì»¨í…Œì´ë„ˆ
            "img[alt*='celebrity']",  # ì…€ëŸ½ ì´ë¯¸ì§€
            "img[alt*='profile']",  # í”„ë¡œí•„ ì´ë¯¸ì§€
            "img[alt*='actor']",  # ë°°ìš° ì´ë¯¸ì§€
            "img[alt*='actress']",  # ì—¬ë°°ìš° ì´ë¯¸ì§€
            "img[alt*='star']",  # ìŠ¤íƒ€ ì´ë¯¸ì§€
            "img[alt*='famous']",  # ìœ ëª…ì¸ ì´ë¯¸ì§€
            "img[alt*='japanese']",  # ì¼ë³¸ ê´€ë ¨
            "img[alt*='korean']",  # í•œêµ­ ê´€ë ¨
            "img[alt*='chinese']",  # ì¤‘êµ­ ê´€ë ¨
        ]
        
        all_elements = []
        for selector in selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    print(f"âœ… {selector}ë¡œ {len(elements)}ê°œ ì´ë¯¸ì§€ ìš”ì†Œ ë°œê²¬")
                    all_elements.extend(elements)
            except Exception as e:
                print(f"âš ï¸ {selector} ì„ íƒì ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        unique_elements = []
        seen_urls = set()
        for element in all_elements:
            try:
                url = element.get_attribute('src')
                if url and url not in seen_urls:
                    unique_elements.append(element)
                    seen_urls.add(url)
            except:
                continue
        
        print(f"âœ… ì´ {len(unique_elements)}ê°œ ê³ ìœ  ì´ë¯¸ì§€ ìš”ì†Œ ë°œê²¬")
        return unique_elements
    
    def extract_image_url(self, img_element):
        """ì´ë¯¸ì§€ URL ì¶”ì¶œ - ì¸ë„¤ì¼ì—ì„œ ì›ë³¸ URL ì¶”ì¶œ"""
        try:
            # ì¸ë„¤ì¼ URL í™•ì¸
            thumbnail_url = img_element.get_attribute('src')
            
            # Google ì¸ë„¤ì¼ URLì—ì„œ ì›ë³¸ URL ì¶”ì¶œ
            if thumbnail_url and 'gstatic.com' in thumbnail_url:
                # ì¸ë„¤ì¼ URLì—ì„œ ì›ë³¸ URL íŒ¨í„´ ì¶”ì¶œ
                if 'encrypted-tbn0.gstatic.com' in thumbnail_url:
                    # ì¸ë„¤ì¼ URLì—ì„œ ì›ë³¸ URL ì¶”ì¶œ ì‹œë„
                    try:
                        # JavaScriptë¡œ ì›ë³¸ URL ì¶”ì¶œ
                        original_url = self.driver.execute_script("""
                            var img = arguments[0];
                            var parent = img.closest('a');
                            if (parent) {
                                return parent.href;
                            }
                            return null;
                        """, img_element)
                        
                        if original_url and 'http' in original_url:
                            print(f"âœ… ì›ë³¸ URL ì¶”ì¶œ: {original_url[:50]}...")
                            return original_url
                    except:
                        pass
                
                # ì¸ë„¤ì¼ URL ìì²´ ì‚¬ìš© (Google ì´ë¯¸ì§€ì¸ ê²½ìš°)
                if 'gstatic.com' in thumbnail_url and 'encrypted-tbn' in thumbnail_url:
                    print(f"âš ï¸ Google ì¸ë„¤ì¼ URL ì‚¬ìš©: {thumbnail_url[:50]}...")
                    return thumbnail_url
            
            # ì´ë¯¸ì§€ í´ë¦­ ì‹œë„ (ë” ì•ˆì „í•œ ë°©ë²•)
            try:
                # JavaScriptë¡œ í´ë¦­
                self.driver.execute_script("arguments[0].click();", img_element)
                time.sleep(2)
                
                # í° ì´ë¯¸ì§€ ì°¾ê¸°
                large_img_selectors = [
                    "img.r48jcc",  # Google ì´ë¯¸ì§€ ë·°ì–´ì˜ í° ì´ë¯¸ì§€
                    "img[src*='gstatic.com']",  # Google ì •ì  ì´ë¯¸ì§€
                    "img[src*='googleusercontent.com']",  # Google ì‚¬ìš©ì ì½˜í…ì¸ 
                    "img[src*='http']",  # HTTP ì´ë¯¸ì§€
                    "img[data-src*='http']",  # ì§€ì—° ë¡œë”© ì´ë¯¸ì§€
                    ".tvh9oe img",  # Google ì´ë¯¸ì§€ ë·°ì–´
                    ".v4dQwb img",  # Google ì´ë¯¸ì§€ ë·°ì–´
                    ".n3VNCb",  # Google ì´ë¯¸ì§€ í´ë˜ìŠ¤
                ]
                
                for selector in large_img_selectors:
                    try:
                        large_img = WebDriverWait(self.driver, 3).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                        )
                        url = large_img.get_attribute('src')
                        if url and url.startswith('http') and not url.endswith('.gif'):
                            if url != thumbnail_url:
                                print(f"âœ… í´ë¦­ í›„ ì›ë³¸ URL ë°œê²¬: {url[:50]}...")
                                return url
                    except TimeoutException:
                        continue
                    except Exception as e:
                        continue
                        
            except Exception as e:
                print(f"âš ï¸ ì´ë¯¸ì§€ í´ë¦­ ì‹¤íŒ¨: {e}")
            
            # ì¸ë„¤ì¼ URL ì‚¬ìš© (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
            if thumbnail_url and thumbnail_url.startswith('http'):
                print(f"âš ï¸ ì¸ë„¤ì¼ URL ì‚¬ìš©: {thumbnail_url[:50]}...")
                return thumbnail_url
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def download_image(self, url, save_path):
        """ê³ í™”ì§ˆ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ê²€ì¦"""
        try:
            # ì˜ëª»ëœ URL í•„í„°ë§
            if 'fonts.gstatic.com' in url or 'productlogos' in url:
                print(f"âš ï¸ ì˜ëª»ëœ ì´ë¯¸ì§€ URL ì œì™¸: {url[:50]}...")
                return False
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://www.google.com/',
                'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Accept-Language': 'en-US,en;q=0.9',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache'
            }
            
            response = requests.get(url, headers=headers, timeout=20)
            response.raise_for_status()
            
            # ì´ë¯¸ì§€ ê²€ì¦
            img = Image.open(io.BytesIO(response.content))
            
            # í¬ê¸° ê¸°ì¤€ ë”ìš± ì™„í™” (ìµœì†Œ 80px)
            min_size = min(img.width, img.height)
            if min_size < 80:
                print(f"âš ï¸ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìŒ: {img.width}x{img.height} (ìµœì†Œ: 80px)")
                return False
            
            # ë„ˆë¬´ ê°€ë¡œì„¸ë¡œ ë¹„ìœ¨ì´ ê·¹ë‹¨ì ì¸ ì´ë¯¸ì§€ ì œì™¸ (1:6 ì´ìƒ)
            ratio = max(img.width, img.height) / min(img.width, img.height)
            if ratio > 6:
                print(f"âš ï¸ ì´ë¯¸ì§€ ë¹„ìœ¨ì´ ë„ˆë¬´ ê·¹ë‹¨ì : {img.width}x{img.height} (ë¹„ìœ¨: {ratio:.1f})")
                return False
            
            # ì›ë³¸ í¬ê¸° ìœ ì§€ (ë¦¬ì‚¬ì´ì¦ˆí•˜ì§€ ì•ŠìŒ)
            # ê³ í’ˆì§ˆë¡œ ì €ì¥
            img.save(save_path, 'JPEG', quality=95, optimize=True)
            
            print(f"âœ… ì´ë¯¸ì§€ ì €ì¥: {img.width}x{img.height} (ìµœì†Œ í¬ê¸°: {min_size}px)")
            return True
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url[:50]}... - {e}")
            return False
    
    def search_and_download(self, query, save_dir, max_images=100):
        """ì´ë¯¸ì§€ ê²€ìƒ‰ ë° ë‹¤ìš´ë¡œë“œ"""
        # ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ ì‹œë„
        search_queries = [
            query,  # ì›ë³¸ ê²€ìƒ‰ì–´
            f"{query} high resolution",  # ê³ í•´ìƒë„
            f"{query} large size",  # í° í¬ê¸°
            f"{query} professional",  # ì „ë¬¸ì 
            f"{query} official",  # ê³µì‹
            f"{query} HD",  # HD
            f"{query} HQ",  # ê³ í’ˆì§ˆ
        ]
        
        total_downloaded = 0
        
        for search_query in search_queries:
            if total_downloaded >= max_images:
                break
                
            search_url = f"https://www.google.com/search?q={search_query}&tbm=isch&hl=en"
            
            try:
                print(f"ğŸ” ê²€ìƒ‰ URL: {search_url}")
                self.driver.get(search_url)
                time.sleep(3)
                
                # í˜ì´ì§€ ìŠ¤í¬ë¡¤ (ë” ë§ì´)
                self.scroll_page(max_scrolls=8)
                
                # ì´ë¯¸ì§€ ìš”ì†Œ ì°¾ê¸°
                img_elements = self.find_image_elements()
                
                if not img_elements:
                    print("âŒ ì´ë¯¸ì§€ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                os.makedirs(save_dir, exist_ok=True)
                
                for i, img in enumerate(img_elements):
                    if total_downloaded >= max_images:
                        break
                        
                    try:
                        print(f"ğŸ“¸ ì´ë¯¸ì§€ {total_downloaded + 1}/{max_images} ì²˜ë¦¬ ì¤‘...")
                        
                        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                        image_url = self.extract_image_url(img)
                        
                        if image_url:
                            # íŒŒì¼ëª… ìƒì„±
                            filename = f"{query.replace(' ', '_')}_{total_downloaded + 1:03d}.jpg"
                            save_path = os.path.join(save_dir, filename)
                            
                            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                            if self.download_image(image_url, save_path):
                                total_downloaded += 1
                                print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename}")
                            else:
                                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {filename}")
                        else:
                            print(f"âš ï¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ ì‹¤íŒ¨")
                        
                        # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                        time.sleep(0.5)  # ê°„ê²© ì¤„ì„
                        
                    except Exception as e:
                        print(f"âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                
                print(f"âœ… '{search_query}' ê²€ìƒ‰ìœ¼ë¡œ {total_downloaded}ì¥ ìˆ˜ì§‘")
                
                # ì¶©ë¶„í•œ ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘í–ˆìœ¼ë©´ ë‹¤ìŒ ê²€ìƒ‰ì–´ë¡œ ë„˜ì–´ê°€ì§€ ì•ŠìŒ
                if total_downloaded >= max_images:
                    break
                    
            except Exception as e:
                print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        return total_downloaded
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ")


def get_priority_countries():
    """2ë‹¨ê³„ í™•ì¥ êµ­ê°€ ëª©ë¡ ë°˜í™˜"""
    return [
        # ìœ ëŸ½ í™•ì¥ (21-30)
        ("swedish", "female"), ("swedish", "male"),
        ("norwegian", "female"), ("norwegian", "male"),
        ("danish", "female"), ("danish", "male"),
        ("polish", "female"), ("polish", "male"),
        ("czech", "female"), ("czech", "male"),
        ("dutch", "female"), ("dutch", "male"),
        ("belgian", "female"), ("belgian", "male"),
        ("swiss", "female"), ("swiss", "male"),
        ("austrian", "female"), ("austrian", "male"),
        ("irish", "female"), ("irish", "male"),
        
        # ë™ë‚¨ì•„ì‹œì•„ í™•ì¥ (31-34)
        ("vietnamese", "female"), ("vietnamese", "male"),
        ("filipino", "female"), ("filipino", "male"),
        ("malaysian", "female"), ("malaysian", "male"),
        ("singaporean", "female"), ("singaporean", "male"),
        
        # ì¤‘ë‚¨ë¯¸ í™•ì¥ (35)
        ("argentine", "female"), ("argentine", "male"),
        
        # ì¤‘ë™ í™•ì¥ (36-40)
        ("saudi", "female"), ("saudi", "male"),
        ("egyptian", "female"), ("egyptian", "male"),
        ("lebanese", "female"), ("lebanese", "male"),
        ("jordanian", "female"), ("jordanian", "male"),
        ("emirati", "female"), ("emirati", "male"),
    ]

def save_progress(completed_countries, total_countries):
    """ì§„í–‰ ìƒí™© ì €ì¥"""
    try:
        progress = {
            "completed": completed_countries,
            "total": total_countries,
            "timestamp": time.time()
        }
        
        with open("/content/progress.json", "w") as f:
            import json
            json.dump(progress, f, indent=2)
        
        print(f"ğŸ’¾ ì§„í–‰ ìƒí™© ì €ì¥: {len(completed_countries)}/{len(total_countries)} ì™„ë£Œ")
        
    except Exception as e:
        print(f"âš ï¸ ì§„í–‰ ìƒí™© ì €ì¥ ì‹¤íŒ¨: {e}")

def load_progress():
    """ì§„í–‰ ìƒí™© ë¡œë“œ"""
    try:
        if os.path.exists("/content/progress.json"):
            with open("/content/progress.json", "r") as f:
                import json
                progress = json.load(f)
            
            print(f"ğŸ“‚ ì§„í–‰ ìƒí™© ë³µêµ¬: {len(progress['completed'])}/{len(progress['total'])} ì™„ë£Œ")
            return progress['completed'], progress['total']
        
    except Exception as e:
        print(f"âš ï¸ ì§„í–‰ ìƒí™© ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return [], []

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸš€ ê³ í™”ì§ˆ Colab í¬ë¡¤ëŸ¬ v4 ì‹œì‘ (2ë‹¨ê³„ í™•ì¥ êµ­ê°€)")
    
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = HighQualityColabCrawler()
    
    try:
        # 2ë‹¨ê³„ í™•ì¥ êµ­ê°€ ëª©ë¡
        priority_countries = get_priority_countries()
        
        # ì§„í–‰ ìƒí™© ë¡œë“œ
        completed_countries, _ = load_progress()
        
        base_path = "/content/dataset"
        total_downloaded = 0
        
        print(f"ğŸ“‹ ì´ {len(priority_countries)}ê°œ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì˜ˆì •")
        
        for i, (country, gender) in enumerate(priority_countries):
            # ì´ë¯¸ ì™„ë£Œëœ êµ­ê°€ ê±´ë„ˆë›°ê¸°
            country_key = f"{country}_{gender}"
            if country_key in completed_countries:
                print(f"â­ï¸ [{i+1}/{len(priority_countries)}] {country} {gender} ì´ë¯¸ ì™„ë£Œë¨")
                continue
            
            print(f"\nğŸ“¸ [{i+1}/{len(priority_countries)}] {country} {gender} ìˆ˜ì§‘ ì¤‘...")
            
            # ê²€ìƒ‰ì–´ ìƒì„±
            query = f"{country} {gender} celebrity profile picture"
            
            # ì €ì¥ ê²½ë¡œ ì„¤ì •
            save_dir = os.path.join(base_path, gender, country.replace(' ', '_'))
            
            downloaded = crawler.search_and_download(query, save_dir, max_images=100)
            total_downloaded += downloaded
            
            print(f"âœ… {country} {gender}: {downloaded}ì¥ ìˆ˜ì§‘ ì™„ë£Œ")
            
            # ì§„í–‰ ìƒí™© ì €ì¥
            completed_countries.append(country_key)
            save_progress(completed_countries, priority_countries)
            
            # êµ­ê°€ ê°„ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(5)
        
        print(f"\nğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {total_downloaded}ì¥")
        
        # ê²°ê³¼ í™•ì¸
        if os.path.exists(base_path):
            print("\nğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°:")
            for root, dirs, files in os.walk(base_path):
                if files:
                    print(f"  {root}: {len(files)}ê°œ íŒŒì¼")
        
        # ğŸ”¥ ìë™ ì••ì¶• ë° ë‹¤ìš´ë¡œë“œ ì¶”ê°€
        if os.path.exists(base_path) and total_downloaded > 0:
            print("\nğŸ“¦ ìë™ ì••ì¶• ë° ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            auto_compress_and_download(base_path, total_downloaded)
        
    finally:
        crawler.close()

def auto_compress_and_download(dataset_path, total_files):
    """ìë™ ì••ì¶• ë° ë‹¤ìš´ë¡œë“œ"""
    try:
        import zipfile
        
        zip_path = "/content/dataset.zip"
        
        # ê¸°ì¡´ ì••ì¶• íŒŒì¼ ì‚­ì œ
        if os.path.exists(zip_path):
            os.remove(zip_path)
            print("ğŸ—‘ï¸ ê¸°ì¡´ ì••ì¶• íŒŒì¼ ì‚­ì œ")
        
        print("ğŸ“¦ ë°ì´í„°ì…‹ ì••ì¶• ì¤‘...")
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files_list in os.walk(dataset_path):
                for file in files_list:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, dataset_path)
                    zipf.write(file_path, arcname)
        
        # ì••ì¶• íŒŒì¼ í¬ê¸° í™•ì¸
        zip_size = os.path.getsize(zip_path) / (1024 * 1024)  # MB
        print(f"âœ… ì••ì¶• ì™„ë£Œ: {zip_path} ({zip_size:.1f} MB)")
        
        # ì•ˆì „í•œ ë‹¤ìš´ë¡œë“œ ì‹œë„
        try:
            from google.colab import files
            print("ğŸ“¥ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            files.download(zip_path)
            print("ğŸ‰ ìë™ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            print(f"ğŸ“ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼: dataset.zip ({zip_size:.1f} MB)")
            print(f"ğŸ“Š ì´ {total_files}ê°œ íŒŒì¼ì´ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as download_error:
            print(f"âš ï¸ ìë™ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {download_error}")
            print("\nğŸ’¡ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë°©ë²•:")
            print("1. ë‹¤ìŒ ì½”ë“œë¥¼ ìƒˆ ì…€ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”:")
            print("   from google.colab import files")
            print("   files.download('/content/dataset.zip')")
            print("2. ë˜ëŠ” ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ:")
            print("   - íŒŒì¼ íƒìƒ‰ê¸°ì—ì„œ /content/dataset.zip íŒŒì¼ì„ ì°¾ì•„ ë‹¤ìš´ë¡œë“œ")
            print("3. ë˜ëŠ” ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ë‹¤ìš´ë¡œë“œ:")
            print("   !wget --content-disposition /content/dataset.zip")
        
    except Exception as e:
        print(f"âŒ ìë™ ì••ì¶•/ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

# íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •
if __name__ == "__main__":
    install_requirements()
    setup_environment()
    main() 