#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Colab í¬ë¡¤ëŸ¬ v5 - ëŒ€ê·œëª¨ ë°ì´í„° ìˆ˜ì§‘ & Google Drive ìë™ ì €ì¥ (ìˆ˜ì •ë¨)
13ê°œ êµ­ê°€ ì—¬ì„± ë°ì´í„° ê°ê° ìµœì†Œ 300ì¥ ìˆ˜ì§‘
"""

import os
import sys
import subprocess
import time
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from PIL import Image
import io
import re
import zipfile
from google.colab import drive

def install_requirements():
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜"""
    print("ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...")
    
    packages = [
        "selenium==4.15.2",
        "pillow==10.1.0", 
        "requests==2.31.0",
        "webdriver-manager==4.0.1"
    ]
    
    for package in packages:
        try:
            subprocess.run([sys.executable, "-m", "pip", "install", package], 
                         check=True, capture_output=True)
            print(f"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ")
        except subprocess.CalledProcessError as e:
            print(f"âŒ {package} ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
    
    # Chrome ë“œë¼ì´ë²„ ì„¤ì¹˜
    try:
        subprocess.run(["apt-get", "update"], check=True, capture_output=True)
        subprocess.run(["apt-get", "install", "-y", "chromium-chromedriver"], 
                      check=True, capture_output=True)
        print("âœ… Chrome ë“œë¼ì´ë²„ ì„¤ì¹˜ ì™„ë£Œ")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Chrome ë“œë¼ì´ë²„ ì„¤ì¹˜ ì‹¤íŒ¨: {e}")

def setup_google_drive():
    """Google Drive ë§ˆìš´íŠ¸"""
    print("ğŸ”— Google Drive ì—°ê²° ì¤‘...")
    try:
        drive.mount('/content/drive')
        print("âœ… Google Drive ì—°ê²° ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"âŒ Google Drive ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

def setup_environment():
    """í™˜ê²½ ì„¤ì •"""
    os.environ['PATH'] += ':/usr/bin/chromedriver'
    
    try:
        import torch
        if torch.cuda.is_available():
            print("ğŸš€ GPU ê°€ì† ì‚¬ìš© ê°€ëŠ¥")
        else:
            print("âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    except ImportError:
        print("âš ï¸ PyTorch ë¯¸ì„¤ì¹˜ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")

class LargeScaleColabCrawler:
    def __init__(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.driver = None
        self.setup_driver()
    
    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì • - ì•ˆì •ì„± ìµœì í™”"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # ì•ˆì •ì„±ì„ ìœ„í•œ ì¶”ê°€ ì˜µì…˜
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-images')  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
        chrome_options.add_argument('--disable-javascript')  # JavaScript ë¹„í™œì„±í™”ë¡œ ì•ˆì •ì„± í–¥ìƒ
        chrome_options.add_argument('--disable-web-security')
        chrome_options.add_argument('--allow-running-insecure-content')
        chrome_options.add_argument('--disable-features=VizDisplayCompositor')
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ ì„¤ì •
            self.driver.set_page_load_timeout(30)
            self.driver.implicitly_wait(10)
            
            print("âœ… Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ (ì•ˆì •ì„± ìµœì í™”)")
        except Exception as e:
            print(f"âŒ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def scroll_page(self, max_scrolls=15):
        """í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ì´ë¯¸ì§€ ë¡œë“œ"""
        for i in range(max_scrolls):
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            print(f"ğŸ“œ ìŠ¤í¬ë¡¤ {i+1}/{max_scrolls}")
    
    def find_image_elements(self):
        """ë‹¤ì–‘í•œ CSS ì„ íƒìë¡œ ì´ë¯¸ì§€ ìš”ì†Œ ì°¾ê¸°"""
        selectors = [
            "img.rg_i",  # ê¸°ì¡´ ì„ íƒì
            "img[data-src]",  # ì§€ì—° ë¡œë”© ì´ë¯¸ì§€
            "img[src*='http']",  # HTTP ì´ë¯¸ì§€
            ".rg_i",  # ì´ë¯¸ì§€ ì»¨í…Œì´ë„ˆ
            "img[alt*='celebrity']",  # ì…€ëŸ½ ì´ë¯¸ì§€
            "img[alt*='profile']",  # í”„ë¡œí•„ ì´ë¯¸ì§€
            "img[alt*='actor']",  # ë°°ìš° ì´ë¯¸ì§€
            "img[alt*='actress']",  # ì—¬ë°°ìš° ì´ë¯¸ì§€
            "img[alt*='star']",  # ìŠ¤íƒ€ ì´ë¯¸ì§€
            "img[alt*='famous']",  # ìœ ëª…ì¸ ì´ë¯¸ì§€
            "img[alt*='portrait']",  # ì´ˆìƒí™”
            "img[alt*='face']",  # ì–¼êµ´
            "img[alt*='woman']",  # ì—¬ì„±
            "img[alt*='female']",  # ì—¬ì„±
            "img[alt*='influencer']",  # ì¸í”Œë£¨ì–¸ì„œ
            "img[alt*='model']",  # ëª¨ë¸
        ]
        
        all_elements = []
        for selector in selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    print(f"âœ… {selector}ë¡œ {len(elements)}ê°œ ì´ë¯¸ì§€ ìš”ì†Œ ë°œê²¬")
                    all_elements.extend(elements)
            except Exception as e:
                print(f"âš ï¸ {selector} ì„ íƒì ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        unique_elements = []
        seen_urls = set()
        for element in all_elements:
            try:
                url = element.get_attribute('src')
                if url and url not in seen_urls:
                    unique_elements.append(element)
                    seen_urls.add(url)
            except:
                continue
        
        print(f"âœ… ì´ {len(unique_elements)}ê°œ ê³ ìœ  ì´ë¯¸ì§€ ìš”ì†Œ ë°œê²¬")
        return unique_elements
    
    def extract_image_url(self, img_element):
        """ì›ë³¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ - ì™„ì „íˆ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹"""
        try:
            # ì¸ë„¤ì¼ URL í™•ì¸
            thumbnail_url = img_element.get_attribute('src')
            
            # ì˜ëª»ëœ URL í•„í„°ë§
            if not thumbnail_url or not thumbnail_url.startswith('http'):
                return None
            
            # Google ì¸ë„¤ì¼ URL ì²˜ë¦¬ - ì›ë³¸ URL ì¶”ì¶œ
            if 'gstatic.com' in thumbnail_url and 'encrypted-tbn' in thumbnail_url:
                # ë°©ë²• 1: ë¶€ëª¨ ë§í¬ì—ì„œ ì›ë³¸ URL ì¶”ì¶œ
                try:
                    parent_link = img_element.find_element(By.XPATH, "./..")
                    if parent_link.tag_name == 'a':
                        original_url = parent_link.get_attribute('href')
                        if original_url and 'http' in original_url and not original_url.endswith('.gif'):
                            print(f"âœ… ì›ë³¸ URL ì¶”ì¶œ (ë¶€ëª¨ ë§í¬): {original_url[:50]}...")
                            return original_url
                except:
                    pass
                
                # ë°©ë²• 2: JavaScriptë¡œ ì›ë³¸ URL ì¶”ì¶œ
                try:
                    original_url = self.driver.execute_script("""
                        var img = arguments[0];
                        var parent = img.closest('a');
                        if (parent && parent.href) {
                            return parent.href;
                        }
                        return null;
                    """, img_element)
                    
                    if original_url and 'http' in original_url and not original_url.endswith('.gif'):
                        print(f"âœ… ì›ë³¸ URL ì¶”ì¶œ (JavaScript): {original_url[:50]}...")
                        return original_url
                except:
                    pass
                
                # ë°©ë²• 3: ì´ë¯¸ì§€ í´ë¦­ìœ¼ë¡œ ì›ë³¸ URL ì¶”ì¶œ (ìµœì í™”)
                try:
                    # JavaScriptë¡œ í´ë¦­
                    self.driver.execute_script("arguments[0].click();", img_element)
                    time.sleep(3)  # ëŒ€ê¸° ì‹œê°„ ìµœì í™”
                    
                    # í˜„ì¬ í˜ì´ì§€ì˜ URL í™•ì¸
                    current_url = self.driver.current_url
                    if 'imgres' in current_url or 'search' in current_url:
                        # ì´ë¯¸ì§€ ë·°ì–´ í˜ì´ì§€ì—ì„œ í° ì´ë¯¸ì§€ ì°¾ê¸° (ë¹ ë¥¸ ì„ íƒìë§Œ)
                        large_img_selectors = [
                            "img.r48jcc",  # Google ì´ë¯¸ì§€ ë·°ì–´ì˜ í° ì´ë¯¸ì§€
                            "img[src*='http']",  # HTTP ì´ë¯¸ì§€
                            ".n3VNCb",  # Google ì´ë¯¸ì§€ í´ë˜ìŠ¤
                            "img[src*='usercontent']",  # ì‚¬ìš©ì ì½˜í…ì¸ 
                        ]
                        
                        for selector in large_img_selectors:
                            try:
                                large_imgs = self.driver.find_elements(By.CSS_SELECTOR, selector)
                                for large_img in large_imgs:
                                    url = large_img.get_attribute('src')
                                    if url and url.startswith('http') and not url.endswith('.gif'):
                                        if url != thumbnail_url and 'gstatic.com' not in url:
                                            print(f"âœ… í´ë¦­ í›„ ì›ë³¸ URL ë°œê²¬: {url[:50]}...")
                                            return url
                            except Exception as e:
                                continue
                        
                        # í˜ì´ì§€ì—ì„œ ì§ì ‘ ì´ë¯¸ì§€ URL ì¶”ì¶œ ì‹œë„ (ë¹ ë¥¸ íŒ¨í„´ë§Œ)
                        try:
                            page_source = self.driver.page_source
                            # ì´ë¯¸ì§€ URL íŒ¨í„´ ì°¾ê¸° (ìµœì í™”ëœ íŒ¨í„´)
                            import re
                            img_patterns = [
                                r'https://[^"\s]+\.(?:jpg|jpeg|png|webp)',
                                r'https://[^"\s]+/images[^"\s]+',
                            ]
                            
                            for pattern in img_patterns:
                                matches = re.findall(pattern, page_source)
                                for match in matches:
                                    if match != thumbnail_url and 'gstatic.com' not in match:
                                        print(f"âœ… í˜ì´ì§€ì—ì„œ ì›ë³¸ URL ë°œê²¬: {match[:50]}...")
                                        return match
                        except:
                            pass
                            
                except Exception as e:
                    print(f"âš ï¸ ì´ë¯¸ì§€ í´ë¦­ ì‹¤íŒ¨: {e}")
                
                # ì¸ë„¤ì¼ URL ìì²´ ì‚¬ìš© (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
                print(f"âš ï¸ Google ì¸ë„¤ì¼ URL ì‚¬ìš©: {thumbnail_url[:50]}...")
                return thumbnail_url
            
            # ì¼ë°˜ ì´ë¯¸ì§€ URL ì²˜ë¦¬
            if thumbnail_url.startswith('http') and not thumbnail_url.endswith('.gif'):
                # ì´ë¯¸ì§€ í™•ì¥ì í™•ì¸
                valid_extensions = ['.jpg', '.jpeg', '.png', '.webp']
                if any(ext in thumbnail_url.lower() for ext in valid_extensions):
                    print(f"âœ… ì§ì ‘ ì´ë¯¸ì§€ URL: {thumbnail_url[:50]}...")
                    return thumbnail_url
            
            # ì¸ë„¤ì¼ URL ì‚¬ìš© (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
            if thumbnail_url and thumbnail_url.startswith('http'):
                print(f"âš ï¸ ì¸ë„¤ì¼ URL ì‚¬ìš©: {thumbnail_url[:50]}...")
                return thumbnail_url
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def download_image(self, url, save_path):
        """ê³ í™”ì§ˆ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ê²€ì¦ - ì™„ì „íˆ ê°œì„ ëœ ë²„ì „"""
        try:
            # ì˜ëª»ëœ URL í•„í„°ë§
            if not url or not url.startswith('http'):
                print(f"âš ï¸ ì˜ëª»ëœ URL í˜•ì‹: {url}")
                return False
            
            # Google ì¸ë„¤ì¼ URL í•„í„°ë§ (ì›ë³¸ ì´ë¯¸ì§€ë§Œ ë‹¤ìš´ë¡œë“œ)
            if 'gstatic.com' in url and 'encrypted-tbn' in url:
                print(f"âš ï¸ Google ì¸ë„¤ì¼ URL ì œì™¸ (ì›ë³¸ ì´ë¯¸ì§€ í•„ìš”): {url[:50]}...")
                return False
            
            if any(bad in url.lower() for bad in ['fonts.gstatic.com', 'productlogos', 'favicon', 'logo']):
                print(f"âš ï¸ ì˜ëª»ëœ ì´ë¯¸ì§€ URL ì œì™¸: {url[:50]}...")
                return False
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://www.google.com/',
                'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Accept-Language': 'en-US,en;q=0.9',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache'
            }
            
            response = requests.get(url, headers=headers, timeout=30)  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
            response.raise_for_status()
            
            # Content-Type í™•ì¸
            content_type = response.headers.get('content-type', '').lower()
            if not content_type.startswith('image/'):
                print(f"âš ï¸ ì´ë¯¸ì§€ê°€ ì•„ë‹Œ íŒŒì¼: {content_type}")
                return False
            
            # ì´ë¯¸ì§€ ê²€ì¦
            try:
                img = Image.open(io.BytesIO(response.content))
            except Exception as e:
                print(f"âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ì´ ì•„ë‹˜: {e}")
                return False
            
            # í¬ê¸° ê¸°ì¤€ ë”ìš± ì™„í™” (ìµœì†Œ 100pxë¡œ ë‹¤ì‹œ ìƒí–¥)
            min_size = min(img.width, img.height)
            if min_size < 100:
                print(f"âš ï¸ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìŒ: {img.width}x{img.height} (ìµœì†Œ: 100px)")
                return False
            
            # ë„ˆë¬´ ê°€ë¡œì„¸ë¡œ ë¹„ìœ¨ì´ ê·¹ë‹¨ì ì¸ ì´ë¯¸ì§€ ì œì™¸ (1:5 ì´ìƒ)
            ratio = max(img.width, img.height) / min(img.width, img.height)
            if ratio > 5:
                print(f"âš ï¸ ì´ë¯¸ì§€ ë¹„ìœ¨ì´ ë„ˆë¬´ ê·¹ë‹¨ì : {img.width}x{img.height} (ë¹„ìœ¨: {ratio:.1f})")
                return False
            
            # ì´ë¯¸ì§€ ëª¨ë“œ í™•ì¸ ë° ë³€í™˜
            if img.mode in ('RGBA', 'LA', 'P'):
                # íˆ¬ëª…ë„ê°€ ìˆëŠ” ì´ë¯¸ì§€ëŠ” RGBë¡œ ë³€í™˜
                if img.mode == 'RGBA':
                    # í°ìƒ‰ ë°°ê²½ì— í•©ì„±
                    background = Image.new('RGB', img.size, (255, 255, 255))
                    background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                    img = background
                else:
                    img = img.convert('RGB')
            
            # ê³ í’ˆì§ˆë¡œ ì €ì¥ (quality íŒŒë¼ë¯¸í„° ë¬¸ì œ ì™„ì „ í•´ê²°)
            try:
                # quality íŒŒë¼ë¯¸í„° ì—†ì´ ì €ì¥
                img.save(save_path, 'JPEG', optimize=True)
            except Exception as e:
                # JPEG ì‹¤íŒ¨ ì‹œ PNGë¡œ ì‹œë„
                try:
                    save_path_png = save_path.replace('.jpg', '.png')
                    img.save(save_path_png, 'PNG')
                    print(f"âœ… PNGë¡œ ì €ì¥: {save_path_png}")
                    return True
                except Exception as e2:
                    print(f"âš ï¸ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e2}")
                    return False
            
            print(f"âœ… ì´ë¯¸ì§€ ì €ì¥: {img.width}x{img.height} (ìµœì†Œ í¬ê¸°: {min_size}px)")
            return True
            
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {url[:50]}... - {e}")
            return False
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url[:50]}... - {e}")
            return False
    
    def get_search_queries(self, country):
        """êµ­ê°€ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        base_queries = [
            f"famous {country} women portrait",
            f"{country} women influencer",
            f"{country} female face photo",
            f"{country} women celebrity",
            f"{country} female model",
            f"{country} women actress",
            f"{country} female beauty",
            f"{country} women professional",
            f"{country} female profile",
            f"{country} women star",
            f"{country} female public figure",
            f"{country} women personality",
            f"{country} female influencer",
            f"{country} women model",
            f"{country} female celebrity",
        ]
        
        # íŠ¹ë³„í•œ êµ­ê°€ë³„ ì¿¼ë¦¬ ì¶”ê°€
        special_queries = {
            "korean": [
                "famous korean women portrait",
                "korean women influencer", 
                "korean female face photo",
                "korean women portrait",
            ],
            "japanese": [
                "japanese women jpop",
                "japanese female actress",
                "japanese women beauty",
                "japanese female model",
                "japanese women influencer",
            ],
            "chinese": [
                "chinese women actress",
                "chinese female model",
                "chinese women beauty",
                "chinese female celebrity",
                "chinese women influencer",
            ],
            "indian": [
                "indian women bollywood",
                "indian female actress",
                "indian women model",
                "indian female beauty",
                "indian women influencer",
            ],
            "saudi": [
                "saudi arabian women",
                "saudi female model",
                "saudi women influencer",
                "saudi arabian female",
                "saudi women beauty",
            ],
            "turkish": [
                "turkish women actress",
                "turkish female model",
                "turkish women beauty",
                "turkish female celebrity",
                "turkish women influencer",
            ],
            "russian": [
                "russian women model",
                "russian female actress",
                "russian women beauty",
                "russian female celebrity",
                "russian women influencer",
            ],
            "french": [
                "french women model",
                "french female actress",
                "french women beauty",
                "french female celebrity",
                "french women influencer",
            ],
            "british": [
                "british women actress",
                "british female model",
                "british women beauty",
                "british female celebrity",
                "british women influencer",
            ],
            "nigerian": [
                "nigerian women model",
                "nigerian female actress",
                "nigerian women beauty",
                "nigerian female celebrity",
                "nigerian women influencer",
            ],
            "ethiopian": [
                "ethiopian women model",
                "ethiopian female actress",
                "ethiopian women beauty",
                "ethiopian female celebrity",
                "ethiopian women influencer",
            ],
            "indigenous": [
                "native american women",
                "indigenous american female",
                "native american female model",
                "indigenous women beauty",
                "native american women actress",
            ],
            "mexican": [
                "mexican women actress",
                "mexican female model",
                "mexican women beauty",
                "mexican female celebrity",
                "mexican women influencer",
            ],
        }
        
        # ê¸°ë³¸ ì¿¼ë¦¬ì™€ íŠ¹ë³„ ì¿¼ë¦¬ ê²°í•©
        all_queries = base_queries.copy()
        if country in special_queries:
            all_queries.extend(special_queries[country])
        
        return all_queries
    
    def search_and_download(self, country, save_dir, target_count=300):
        """ì´ë¯¸ì§€ ê²€ìƒ‰ ë° ë‹¤ìš´ë¡œë“œ - ì•ˆì •ì„± ìµœì í™” ë²„ì „"""
        search_queries = self.get_search_queries(country)
        
        total_downloaded = 0
        query_index = 0
        consecutive_failures = 0  # ì—°ì† ì‹¤íŒ¨ íšŸìˆ˜ ì¶”ì 
        
        while total_downloaded < target_count and query_index < len(search_queries):
            search_query = search_queries[query_index]
            query_url = f"https://www.google.com/search?q={search_query}&tbm=isch&hl=en"
            
            try:
                print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")
                print(f"ğŸ“Š í˜„ì¬ ìˆ˜ì§‘: {total_downloaded}/{target_count}")
                
                self.driver.get(query_url)
                time.sleep(3)  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€ (ì•ˆì •ì„±)
                
                # í˜ì´ì§€ ìŠ¤í¬ë¡¤ (ë³´ìˆ˜ì )
                self.scroll_page(max_scrolls=10)  # ìŠ¤í¬ë¡¤ íšŸìˆ˜ ê°ì†Œ
                
                # ì´ë¯¸ì§€ ìš”ì†Œ ì°¾ê¸°
                img_elements = self.find_image_elements()
                
                if not img_elements:
                    print("âŒ ì´ë¯¸ì§€ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    query_index += 1
                    consecutive_failures += 1
                    continue
                
                os.makedirs(save_dir, exist_ok=True)
                
                successful_downloads = 0  # ì´ ì¿¼ë¦¬ì—ì„œ ì„±ê³µí•œ ë‹¤ìš´ë¡œë“œ ìˆ˜
                processed_images = 0  # ì²˜ë¦¬í•œ ì´ë¯¸ì§€ ìˆ˜
                
                for i, img in enumerate(img_elements):
                    if total_downloaded >= target_count:
                        break
                        
                    try:
                        print(f"ğŸ“¸ ì´ë¯¸ì§€ {total_downloaded + 1}/{target_count} ì²˜ë¦¬ ì¤‘...")
                        
                        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                        image_url = self.extract_image_url(img)
                        
                        if image_url:
                            # íŒŒì¼ëª… ìƒì„± (ê¸°ì¡´ ê°œìˆ˜ ë°˜ì˜)
                            next_index = start_index + total_downloaded + 1
                            filename = f"{country}_{next_index:04d}.jpg"
                            save_path = os.path.join(save_dir, filename)
                            
                            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                            if self.download_image(image_url, save_path):
                                total_downloaded += 1
                                successful_downloads += 1
                                consecutive_failures = 0  # ì„±ê³µ ì‹œ ì‹¤íŒ¨ ì¹´ìš´í„° ë¦¬ì…‹
                                print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename}")
                            else:
                                consecutive_failures += 1
                                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {filename}")
                        else:
                            consecutive_failures += 1
                            print(f"âš ï¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ ì‹¤íŒ¨")
                        
                        processed_images += 1
                        
                        # ì—°ì† ì‹¤íŒ¨ê°€ ë§ìœ¼ë©´ ë‹¤ìŒ ì¿¼ë¦¬ë¡œ ë„˜ì–´ê°€ê¸°
                        if consecutive_failures >= 20:  # ì‹¤íŒ¨ ê¸°ì¤€ ë” ë³´ìˆ˜ì 
                            print(f"âš ï¸ ì—°ì† ì‹¤íŒ¨ {consecutive_failures}íšŒ - ë‹¤ìŒ ì¿¼ë¦¬ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤")
                            break
                        
                        # ìš”ì²­ ê°„ê²© ì¡°ì ˆ (ì•ˆì •ì„±)
                        time.sleep(0.5)  # ê°„ê²© ì¦ê°€
                        
                    except Exception as e:
                        consecutive_failures += 1
                        print(f"âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                
                print(f"âœ… '{search_query}' ê²€ìƒ‰ìœ¼ë¡œ {successful_downloads}ì¥ ìˆ˜ì§‘ (ì²˜ë¦¬: {processed_images}ê°œ)")
                query_index += 1
                
                # ì¶©ë¶„í•œ ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘í–ˆìœ¼ë©´ ì¢…ë£Œ
                if total_downloaded >= target_count:
                    break
                    
            except Exception as e:
                print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                consecutive_failures += 1
                query_index += 1
                continue
        
        return total_downloaded
    
    def save_to_google_drive(self, country, local_path):
        """Google Driveì— ì €ì¥ (ì§ì ‘ ì €ì¥ì´ë¯€ë¡œ noop)"""
        try:
            print("â„¹ï¸ ì´ë¯¸ Google Drive ê²½ë¡œì— ì§ì ‘ ì €ì¥ ì¤‘ì…ë‹ˆë‹¤.")
            return True
        except Exception as e:
            print(f"âŒ Google Drive ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ")

def get_target_countries():
    """13ê°œ ì£¼ìš” êµ­ê°€ ëª©ë¡ ë°˜í™˜ - ì „ì²´ êµ­ê°€ í™œì„±í™”"""
    return [
        # "korean",      # ëŒ€í•œë¯¼êµ­
        # "chinese",     # ì¤‘êµ­
        # "japanese",    # ì¼ë³¸
        # "indian",      # ì¸ë„
        # "saudi",       # ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„
        # "turkish",     # í„°í‚¤
        "russian",     # ëŸ¬ì‹œì•„
        "french",      # í”„ë‘ìŠ¤
        "british",     # ì˜êµ­
        "nigerian",    # ë‚˜ì´ì§€ë¦¬ì•„
        "ethiopian",   # ì—í‹°ì˜¤í”¼ì•„
        "indigenous",  # ë¯¸êµ­ ì›ì£¼ë¯¼ ì¸ë””ì–¸
        "mexican",     # ë©•ì‹œì½”
    ]

def save_progress(completed_countries, total_countries):
    """ì§„í–‰ ìƒí™© ì €ì¥"""
    try:
        progress = {
            "completed": completed_countries,
            "total": total_countries,
            "timestamp": time.time()
        }
        
        with open("/content/drive/MyDrive/WhosYourAncestor/progress_v5.json", "w") as f:
            import json
            json.dump(progress, f, indent=2)
        
        print(f"ğŸ’¾ ì§„í–‰ ìƒí™© ì €ì¥: {len(completed_countries)}/{len(total_countries)} ì™„ë£Œ")
        
    except Exception as e:
        print(f"âš ï¸ ì§„í–‰ ìƒí™© ì €ì¥ ì‹¤íŒ¨: {e}")

def load_progress():
    """ì§„í–‰ ìƒí™© ë¡œë“œ"""
    try:
        progress_path = "/content/drive/MyDrive/WhosYourAncestor/progress_v5.json"
        if os.path.exists(progress_path):
            with open(progress_path, "r") as f:
                import json
                progress = json.load(f)
            
            print(f"ğŸ“‚ ì§„í–‰ ìƒí™© ë³µêµ¬: {len(progress['completed'])}/{len(progress['total'])} ì™„ë£Œ")
            return progress['completed'], progress['total']
        
    except Exception as e:
        print(f"âš ï¸ ì§„í–‰ ìƒí™© ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return [], []

def main():
    """ë©”ì¸ ì‹¤í–‰ - ì•ˆì •ì„± ìµœì í™” ë²„ì „"""
    print("ğŸš€ ëŒ€ê·œëª¨ Colab í¬ë¡¤ëŸ¬ v5 ì‹œì‘ (ì•ˆì •ì„± ìµœì í™”)")
    print("ğŸ“Š 13ê°œ êµ­ê°€ ì—¬ì„± ë°ì´í„° ê°ê° 300ì¥ì”© ìˆ˜ì§‘")
    print("ğŸŒ™ ìë™ ì¬ì‹œì‘ ë° ì•ˆì •ì„± ìµœì í™” ëª¨ë“œ")
    
    # Google Drive ì—°ê²°
    if not setup_google_drive():
        print("âŒ Google Drive ì—°ê²° ì‹¤íŒ¨. ë¡œì»¬ì—ë§Œ ì €ì¥í•©ë‹ˆë‹¤.")
    
    # ìë™ ì¬ì‹œì‘ ì¹´ìš´í„°
    restart_count = 0
    max_restarts = 5
    
    while restart_count < max_restarts:
        try:
            # í¬ë¡¤ëŸ¬ ìƒì„±
            crawler = LargeScaleColabCrawler()
            
            # 13ê°œ ì£¼ìš” êµ­ê°€ ëª©ë¡
            target_countries = get_target_countries()
            
            # ì§„í–‰ ìƒí™© ë¡œë“œ
            completed_countries, _ = load_progress()
            
            base_path = "/content/drive/MyDrive/WhosYourAncestor/dataset"
            total_downloaded = 0
            
            print(f"ğŸ“‹ ì´ {len(target_countries)}ê°œ êµ­ê°€ ìˆ˜ì§‘ ì˜ˆì •")
            print(f"ğŸ”„ ì¬ì‹œì‘ íšŸìˆ˜: {restart_count}/{max_restarts}")
            
            for i, country in enumerate(target_countries):
                # ì´ë¯¸ ì™„ë£Œëœ êµ­ê°€ ê±´ë„ˆë›°ê¸°
                if country in completed_countries:
                    print(f"â­ï¸ [{i+1}/{len(target_countries)}] {country} ì´ë¯¸ ì™„ë£Œë¨")
                    continue
                
                print(f"\nğŸ“¸ [{i+1}/{len(target_countries)}] {country} ìˆ˜ì§‘ ì¤‘...")
                
                # ì €ì¥ ê²½ë¡œ ì„¤ì •
                save_dir = os.path.join(base_path, "female", country)
            os.makedirs(save_dir, exist_ok=True)

            # ê¸°ì¡´ íŒŒì¼ ìˆ˜ì— ë§ì¶° ë²ˆí˜¸ ì‹œì‘
            existing = sorted([f for f in os.listdir(save_dir) if f.lower().endswith((".jpg",".png"))])
            start_index = len(existing)
            if start_index > 0:
                print(f"ğŸ”¢ ê¸°ì¡´ {start_index}ê°œ íŒŒì¼ ë°œê²¬ - ë‹¤ìŒ ë²ˆí˜¸ë¶€í„° ì €ì¥")
                
                # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
                max_retries = 3
                downloaded = 0
                
                for retry in range(max_retries):
                    try:
                        print(f"ğŸ”„ ì‹œë„ {retry + 1}/{max_retries}")
                        downloaded = crawler.search_and_download(country, save_dir, target_count=300)
                        
                        if downloaded > 0:
                            print(f"âœ… {country}: {downloaded}ì¥ ìˆ˜ì§‘ ì™„ë£Œ")
                            break
                        else:
                            print(f"âš ï¸ {country}: ìˆ˜ì§‘ ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘...")
                            time.sleep(5)  # ì¬ì‹œë„ ì „ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                            
                    except Exception as e:
                        print(f"âŒ {country} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                        if retry < max_retries - 1:
                            print("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                            time.sleep(10)  # ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                        else:
                            print(f"âŒ {country} ìˆ˜ì§‘ ìµœì¢… ì‹¤íŒ¨")
                
                total_downloaded += downloaded
                
                # Google Driveì— ì €ì¥
                if downloaded > 0:
                    crawler.save_to_google_drive(country, save_dir)
                
                # ì§„í–‰ ìƒí™© ì €ì¥ (ë” ìì£¼)
                completed_countries.append(country)
                save_progress(completed_countries, target_countries)
                
                # êµ­ê°€ ê°„ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€) - ë” ê¸¸ê²Œ
                print(f"â³ ë‹¤ìŒ êµ­ê°€ê¹Œì§€ 30ì´ˆ ëŒ€ê¸°...")
                time.sleep(30)  # ê°„ê²© ëŒ€í­ ì¦ê°€
            
            print(f"\nğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {total_downloaded}ì¥")
            
            # ê²°ê³¼ í™•ì¸
            if os.path.exists(base_path):
                print("\nğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°:")
                for root, dirs, files in os.walk(base_path):
                    if files:
                        print(f"  {root}: {len(files)}ê°œ íŒŒì¼")
            
            # ğŸ”¥ ìë™ ì••ì¶• ë° ë‹¤ìš´ë¡œë“œ ì¶”ê°€
            if os.path.exists(base_path) and total_downloaded > 0:
                print("\nğŸ“¦ ìë™ ì••ì¶• ë° ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
                auto_compress_and_download(base_path, total_downloaded)
            
            # ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ë©´ ë£¨í”„ ì¢…ë£Œ
            break
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
            print("ğŸ’¾ í˜„ì¬ê¹Œì§€ì˜ ì§„í–‰ ìƒí™©ì„ ì €ì¥í•©ë‹ˆë‹¤...")
            save_progress(completed_countries, target_countries)
            print("âœ… ì§„í–‰ ìƒí™© ì €ì¥ ì™„ë£Œ")
            break
            
        except Exception as e:
            restart_count += 1
            print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            print(f"ğŸ”„ ìë™ ì¬ì‹œì‘ {restart_count}/{max_restarts}")
            print("ğŸ’¾ í˜„ì¬ê¹Œì§€ì˜ ì§„í–‰ ìƒí™©ì„ ì €ì¥í•©ë‹ˆë‹¤...")
            save_progress(completed_countries, target_countries)
            print("âœ… ì§„í–‰ ìƒí™© ì €ì¥ ì™„ë£Œ")
            
            if restart_count < max_restarts:
                print("â³ 60ì´ˆ í›„ ìë™ ì¬ì‹œì‘...")
                time.sleep(60)
            else:
                print("âŒ ìµœëŒ€ ì¬ì‹œì‘ íšŸìˆ˜ ì´ˆê³¼")
                break
            
        finally:
            try:
                crawler.close()
            except:
                pass

def auto_compress_and_download(dataset_path, total_files):
    """ìë™ ì••ì¶• ë° ë‹¤ìš´ë¡œë“œ"""
    try:
        import zipfile
        
        zip_path = "/content/dataset_v5.zip"
        
        # ê¸°ì¡´ ì••ì¶• íŒŒì¼ ì‚­ì œ
        if os.path.exists(zip_path):
            os.remove(zip_path)
            print("ğŸ—‘ï¸ ê¸°ì¡´ ì••ì¶• íŒŒì¼ ì‚­ì œ")
        
        print("ğŸ“¦ ë°ì´í„°ì…‹ ì••ì¶• ì¤‘...")
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files_list in os.walk(dataset_path):
                for file in files_list:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, dataset_path)
                    zipf.write(file_path, arcname)
        
        # ì••ì¶• íŒŒì¼ í¬ê¸° í™•ì¸
        zip_size = os.path.getsize(zip_path) / (1024 * 1024)  # MB
        print(f"âœ… ì••ì¶• ì™„ë£Œ: {zip_path} ({zip_size:.1f} MB)")
        
        # ì•ˆì „í•œ ë‹¤ìš´ë¡œë“œ ì‹œë„
        try:
            from google.colab import files
            print("ğŸ“¥ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            files.download(zip_path)
            print("ğŸ‰ ìë™ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            print(f"ğŸ“ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼: dataset_v5.zip ({zip_size:.1f} MB)")
            print(f"ğŸ“Š ì´ {total_files}ê°œ íŒŒì¼ì´ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as download_error:
            print(f"âš ï¸ ìë™ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {download_error}")
            print("\nğŸ’¡ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë°©ë²•:")
            print("1. ë‹¤ìŒ ì½”ë“œë¥¼ ìƒˆ ì…€ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”:")
            print("   from google.colab import files")
            print("   files.download('/content/dataset_v5.zip')")
            print("2. ë˜ëŠ” ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ:")
            print("   - íŒŒì¼ íƒìƒ‰ê¸°ì—ì„œ /content/dataset_v5.zip íŒŒì¼ì„ ì°¾ì•„ ë‹¤ìš´ë¡œë“œ")
            print("3. ë˜ëŠ” ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ë‹¤ìš´ë¡œë“œ:")
            print("   !wget --content-disposition /content/dataset_v5.zip")
        
    except Exception as e:
        print(f"âŒ ìë™ ì••ì¶•/ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

# íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •
if __name__ == "__main__":
    install_requirements()
    setup_environment()
    main() 